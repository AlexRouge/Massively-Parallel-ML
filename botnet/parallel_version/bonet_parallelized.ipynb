{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Botnet Logistic Regression Classifier - Parallelized Version\n",
    "\n",
    "Parallelized version of readFile, normalize, train and accuracy functions. The algorithm converges during training, the cost value decreases asymptotically at each iteration. The final reported accuracy of 93.09% suggests that the model is performing well on the given dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c123757058d3583c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "\n",
    "The `readFile` function in Python uses PySpark to read data from a specified file, process it, and return it as an RDD. Each row in the file is transformed into a record consisting of a tuple, where the first element is a list of 11 floating-point feature values, and the second element is an integer label (0 or 1). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca17e218746164ee"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def readFile(filename):\n",
    "    \"\"\"\n",
    "    Return an RDD containing the data of filename.\n",
    "    Each example (row) of the file corresponds to one RDD record.\n",
    "    Each record of the RDD is a tuple (X,y). “X” is an array containing the 11\n",
    "    features (float number) of an example\n",
    "    “y” is the 12th column of an example (integer 0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory = os.path.dirname(current_directory)\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    data = sc.textFile(current_directory + \"/\" + filename)\n",
    "    processed_data = data.map(lambda line: line.split(\",\")).map(\n",
    "        lambda cols: ([float(x) for x in cols[:11]], float(cols[11]))\n",
    "    )\n",
    "    return processed_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:21:34.837812Z",
     "start_time": "2023-12-16T16:21:34.561955Z"
    }
   },
   "id": "99e22889a19fb5d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizing\n",
    "\n",
    "The `normalize` function, using PySpark, takes an RDD as input and normalizes its features. It first calculates the total number of samples, then proceeds to compute the sum and sum of squares for each feature across all samples. Utilizing the map and reduce functions, it aggregates these values to calculate the mean and variance for each feature. It then computes the standard deviation, handling cases to avoid division by zero. The mean and standard deviation are broadcasted to all nodes. Finally, it applies normalization to each record (feature vector) in the RDD, adjusting each feature to have a mean of 0 and a standard deviation of 1, and returns the normalized RDD."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8781c6331ee31425"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def normalize(RDD_Xy):\n",
    "    sc = SparkContext.getOrCreate()\n",
    "\n",
    "    number_of_samples = RDD_Xy.count()\n",
    "\n",
    "    # Function to compute sum and sum of squares for each feature\n",
    "    def compute_sum_and_squares(record):\n",
    "        X, _ = record\n",
    "        return (np.array(X), np.array(X) ** 2)\n",
    "\n",
    "    # Aggregate the sum and sum of squares for each feature, and count the examples\n",
    "    sum_squares_count = RDD_Xy.map(compute_sum_and_squares).reduce(\n",
    "        lambda a, b: (a[0] + b[0], a[1] + b[1])\n",
    "    )\n",
    "\n",
    "    # Calculate the mean and variance for each feature\n",
    "    mean = sum_squares_count[0] / number_of_samples\n",
    "    variance = (sum_squares_count[1] / number_of_samples) - mean**2\n",
    "    std_dev = np.sqrt(variance)\n",
    "\n",
    "    # Replace zeros in standard deviation with ones to avoid division by zero\n",
    "    std_dev[std_dev == 0] = 1\n",
    "\n",
    "    # Broadcast the mean and std_dev to all the nodes\n",
    "    broadcast_mean = sc.broadcast(mean)\n",
    "    broadcast_std_dev = sc.broadcast(std_dev)\n",
    "\n",
    "    # Function to normalize features\n",
    "    def normalize_features(record):\n",
    "        X, y = record\n",
    "        X_normalized = (X - broadcast_mean.value) / broadcast_std_dev.value\n",
    "        return (X_normalized, y)\n",
    "\n",
    "    # Normalize each feature and return the new RDD\n",
    "    return RDD_Xy.map(normalize_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:21:51.367999Z",
     "start_time": "2023-12-16T16:21:51.358014Z"
    }
   },
   "id": "cee7cbeacb6a256d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "The `train` function in PySpark performs logistic regression training on an RDD. It iterates through a specified number of iterations, broadcasting weights and bias, computing gradients, and updating the model parameters using gradient descent. Auxiliary functions `compute_gradients` and `compute_cost` calculate gradients and cost for each data point, aiding in weight updates and cost monitoring for each iteration. The function is designed for distributed execution in a Spark environment."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "433861937135872d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    sc = SparkContext.getOrCreate()\n",
    "\n",
    "    # Number of features (assuming all records have the same number of features)\n",
    "    k = len(RDD_Xy.first()[0])\n",
    "    m = RDD_Xy.count()  # Total number of examples\n",
    "\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    w = np.random.rand(k)  # Weight vector\n",
    "    b = np.random.rand()  # Bias term\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Broadcast weights and bias\n",
    "        broadcast_w = sc.broadcast(w)\n",
    "        broadcast_b = sc.broadcast(b)\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = RDD_Xy.map(\n",
    "            lambda x: compute_gradients(x, broadcast_w.value, broadcast_b.value, k)\n",
    "        ).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * ((1 / m) * gradients[0] + (lambda_reg / k) * w)\n",
    "        b -= learning_rate * (\n",
    "                (1 / m) * gradients[1] + ((lambda_reg / (2 * k)) * np.sum(w**2))\n",
    "        )\n",
    "\n",
    "        # Optional: Print cost for monitoring (not recommended for large datasets)\n",
    "        cost = RDD_Xy.map(\n",
    "            lambda x: compute_cost(x, broadcast_w.value, broadcast_b.value, k)\n",
    "        ).reduce(lambda x, y: x + y)\n",
    "        cost = (-1 / m) * cost\n",
    "        cost += (lambda_reg / (2 * k)) * np.sum(w**2)\n",
    "        print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def compute_gradients(record, w, b, k):\n",
    "    X, y = record\n",
    "    z = 0\n",
    "    for i in range(k):\n",
    "        z += X[i] * w[i]\n",
    "    z += b\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    dw = (y_hat - y) * X\n",
    "    db = y_hat - y\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def compute_cost(record, w, b, k):\n",
    "    X, y = record\n",
    "    z = 0\n",
    "    for i in range(k):\n",
    "        z += X[i] * w[i]\n",
    "    z += b\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    cost = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "    return cost\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:22:01.486877Z",
     "start_time": "2023-12-16T16:22:01.483764Z"
    }
   },
   "id": "43e5b7f4f155f5f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict\n",
    "\n",
    "The `predict` function computes a logistic regression prediction for a given feature vector `X` using provided weights `w` and bias `b`. It calculates the linear combination of features and weights, adds the bias, then applies the sigmoid function to determine the probability of the instance belonging to the positive class. Based on this probability, it returns a binary class label (0 or 1)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9723bcc9b2802ea"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(w, b, X):\n",
    "    # Initialize the sum\n",
    "    z = 0\n",
    "    # Iterate over the weights and corresponding features\n",
    "    for i in range(len(w)):\n",
    "        z += w[i] * X[i]\n",
    "        # Add the bias term\n",
    "    z += b\n",
    "\n",
    "    # compact way to calculate it z = np.dot(w, X) + b\n",
    "\n",
    "    # Apply the sigmoid function to get the probability\n",
    "    p = 1 / (1 + np.exp(-z))\n",
    "    # Predict the class label (0 or 1) based on the probability\n",
    "    if p >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:22:07.235238Z",
     "start_time": "2023-12-16T16:22:07.227348Z"
    }
   },
   "id": "ab4cf24a12b00dca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "\n",
    "The `accuracy` function evaluates the performance of a logistic regression model on an RDD dataset by calculating its accuracy. It maps each record in the RDD to 1 or 0, based on whether the model's prediction matches the actual label, using the `predict` function. The function then sums these values using `reduce` to count correct predictions and divides this sum by the total number of records in the RDD to compute the overall accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e4808939a6bc8c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from predict import predict\n",
    "\n",
    "def accuracy(w, b, RDD_xy):\n",
    "    prediction_results = RDD_xy.map(\n",
    "        lambda record: 1 if predict(w, b, record[0]) == record[1] else 0\n",
    "    )\n",
    "\n",
    "    # Step 2: Use reduce to sum up the correct predictions\n",
    "    correctly_classified = prediction_results.reduce(lambda a, b: a + b)\n",
    "\n",
    "    # Step 4: Calculate accuracy\n",
    "    accuracy = correctly_classified / RDD_xy.count()\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:22:12.768941Z",
     "start_time": "2023-12-16T16:22:12.760292Z"
    }
   },
   "id": "ccceadbe3201fdb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing\n",
    "\n",
    "The following code snippet is used for testing the whole system:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3da22ce1e89265b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/16 17:23:38 WARN BlockManager: Task 18 already completed, not releasing lock for rdd_4_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 1.5064344561999052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 0.7455361685980132\n",
      "Iteration 2, Cost: 0.4491058666358347\n",
      "Iteration 3, Cost: 0.3418193988664206\n",
      "Iteration 4, Cost: 0.29398719162471226\n",
      "Iteration 5, Cost: 0.2681576630981871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Cost: 0.25226351106148315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Cost: 0.24159520691566388\n",
      "Iteration 8, Cost: 0.23399108125822288\n",
      "Iteration 9, Cost: 0.22833027836928302\n",
      "accuracy: 0.930998\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from accuracy import accuracy\n",
    "from normalize import normalize\n",
    "from preprocess import readFile\n",
    "from pyspark import SparkContext\n",
    "from train import train\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    current_directory = os.getcwd()\n",
    "    sc.addPyFile(current_directory + \"/botnet/parallel_version/\" + \"predict.py\")\n",
    "    sc.addPyFile(current_directory + \"/botnet/parallel_version/\" + \"train.py\")\n",
    "\n",
    "    # read data\n",
    "    data = readFile(\"botnet/data/botnet_tot_syn_l.csv\")\n",
    "    # standardize\n",
    "    data = normalize(data)\n",
    "    # optimize performance\n",
    "    data_cached = data.cache()\n",
    "    # train\n",
    "    weights, bias = train(data_cached, 10, 1.5, 0.05)\n",
    "    # accuracy\n",
    "    accuracy = accuracy(weights, bias, data_cached)\n",
    "    print(\"accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:23:52.978941Z",
     "start_time": "2023-12-16T16:23:34.884507Z"
    }
   },
   "id": "aab4b79255387ba5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dd27e421a46519e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
