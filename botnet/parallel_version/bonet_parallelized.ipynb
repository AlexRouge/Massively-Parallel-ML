{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Botnet Logistic Regression Classifier - Parallelized Version\n",
    "\n",
    "Parallelized version of readFile, normalize, train and accuracy functions. The algorithm converges during training, the cost value decreases asymptotically at each iteration. The final reported accuracy of 92.87% suggests that the model is performing well on the given dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c123757058d3583c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "\n",
    "The `readFile` function in Python uses PySpark to read data from a specified file, process it, and return it as an RDD. Each row in the file is transformed into a record consisting of a tuple, where the first element is a list of 11 floating-point feature values, and the second element is an integer label (0 or 1). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca17e218746164ee"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def readFile(filename):\n",
    "    \"\"\"\n",
    "    Return an RDD containing the data of filename.\n",
    "    Each example (row) of the file corresponds to one RDD record.\n",
    "    Each record of the RDD is a tuple (X,y). “X” is an array containing the 11 features (float number) of an example\n",
    "    “y” is the 12th column of an example (integer 0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory = os.path.dirname(current_directory)\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    data = sc.textFile(parent_directory + \"/\" + filename)\n",
    "    processed_data = data.map(lambda line: line.split(\",\")).map(\n",
    "        lambda cols: ([float(x) for x in cols[:11]], int(cols[11]))\n",
    "    )\n",
    "    return processed_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:02:13.540935Z",
     "start_time": "2023-11-28T14:02:13.320753Z"
    }
   },
   "id": "99e22889a19fb5d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizing\n",
    "\n",
    "The `normalize` function, using PySpark, calculates the mean and standard deviation for each feature in an RDD and normalizes the features to have a mean of 0 and a standard deviation of 1. It involves computing sums and sums of squares of features, broadcasting the calculated mean and standard deviation to all nodes, and applying normalization to each record in the RDD."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8781c6331ee31425"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "\n",
    "def normalize(RDD_Xy):\n",
    "    sc = SparkContext.getOrCreate()\n",
    "\n",
    "    # Function to compute sum and sum of squares for each feature\n",
    "    def compute_sum_and_squares(record):\n",
    "        X, _ = record\n",
    "        return (np.array(X), np.array(X)**2, 1)\n",
    "\n",
    "    # Aggregate the sum and sum of squares for each feature, and count the examples\n",
    "    sum_squares_count = RDD_Xy.map(compute_sum_and_squares).reduce(\n",
    "        lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "\n",
    "    # Calculate the mean and variance for each feature\n",
    "    mean = sum_squares_count[0] / sum_squares_count[2]\n",
    "    variance = (sum_squares_count[1] / sum_squares_count[2]) - mean**2\n",
    "    std_dev = np.sqrt(variance)\n",
    "\n",
    "    # Replace zeros in standard deviation with ones to avoid division by zero\n",
    "    std_dev[std_dev == 0] = 1\n",
    "\n",
    "    # Broadcast the mean and std_dev to all the nodes\n",
    "    broadcast_mean = sc.broadcast(mean)\n",
    "    broadcast_std_dev = sc.broadcast(std_dev)\n",
    "\n",
    "    # Function to normalize features\n",
    "    def normalize_features(record):\n",
    "        X, y = record\n",
    "        X_normalized = (X - broadcast_mean.value) / broadcast_std_dev.value\n",
    "        return (X_normalized, y)\n",
    "\n",
    "    # Normalize each feature and return the new RDD\n",
    "    return RDD_Xy.map(normalize_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:03:06.967132Z",
     "start_time": "2023-11-28T14:03:06.959493Z"
    }
   },
   "id": "cee7cbeacb6a256d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "The `train` function, using PySpark, performs logistic regression training on an RDD dataset. It iterates a specified number of times, updating the model's weights and bias based on computed gradients. In each iteration, it calculates gradients using a distributed approach, updates the weights and bias, and prints the cost for monitoring. The function includes additional helper functions (`compute_gradients` and `compute_cost`) to compute gradients and the cost for each data point, respectively."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "433861937135872d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "\n",
    "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    sc = SparkContext.getOrCreate()\n",
    "\n",
    "    # Number of features (assuming all records have the same number of features)\n",
    "    num_features = len(RDD_Xy.first()[0])\n",
    "    m = RDD_Xy.count()  # Total number of examples\n",
    "\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    w = np.random.rand(num_features)  # Weight vector\n",
    "    b = np.random.rand()  # Bias term\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Broadcast weights and bias\n",
    "        broadcast_w = sc.broadcast(w)\n",
    "        broadcast_b = sc.broadcast(b)\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = RDD_Xy.map(lambda x: compute_gradients(x, broadcast_w.value, broadcast_b.value, m, lambda_reg)) \\\n",
    "            .reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "        # Update weights and bias\n",
    "        w -= learning_rate * gradients[0]\n",
    "        b -= learning_rate * gradients[1]\n",
    "\n",
    "        # Optional: Print cost for monitoring (not recommended for large datasets)\n",
    "        cost = RDD_Xy.map(lambda x: compute_cost(x, broadcast_w.value, broadcast_b.value, lambda_reg, m)) \\\n",
    "            .reduce(lambda x, y: x + y)\n",
    "        print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def compute_gradients(point, w, b, m, lambda_reg):\n",
    "    X, y = point\n",
    "    z = np.dot(X, w) + b\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    dw = (1 / m) * np.dot(X, (y_hat - y)) + (lambda_reg / m) * w\n",
    "    db = (1 / m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def compute_cost(point, w, b, lambda_reg, m):\n",
    "    X, y = point\n",
    "    z = np.dot(X, w) + b\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "    cost = (-1 / m) * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    cost += (lambda_reg / (2 * m)) * np.sum(w ** 2)\n",
    "    return cost"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:05:09.510691Z",
     "start_time": "2023-11-28T14:05:09.501197Z"
    }
   },
   "id": "43e5b7f4f155f5f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict\n",
    "\n",
    "The `predict` function calculates the linear combination of features and weights, adds bias, applies the sigmoid function to derive a probability, and then classifies the input as either class 0 or 1 based on this probability (threshold at 0.5). This function is used for making predictions on individual data examples using trained logistic regression parameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9723bcc9b2802ea"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(w, b, X):\n",
    "    # Compute the linear combination of the weights and the example\n",
    "    z = np.dot(w, X) + b\n",
    "    # Apply the sigmoid function to get the probability\n",
    "    p = 1 / (1 + np.exp(-z))\n",
    "    # Predict the class label (0 or 1) based on the probability\n",
    "    if p >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:05:32.836077Z",
     "start_time": "2023-11-28T14:05:32.821578Z"
    }
   },
   "id": "ab4cf24a12b00dca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "\n",
    "The `accuracy` function, using PySpark, calculates the accuracy of a logistic regression model on an RDD dataset. It maps each record in the RDD to 1 or 0 based on whether the model's prediction matches the actual label, sums up these correct predictions using `reduce`, and then computes the accuracy by dividing the number of correctly classified records by the total number of records in the RDD. The function relies on an external `predict` function to make predictions based on the model's weights (`w`) and bias (`b`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70e4808939a6bc8c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from predict import predict\n",
    "\n",
    "def accuracy(w, b, RDD_xy):\n",
    "    prediction_results = RDD_xy.map(lambda record: 1 if predict(w, b, record[0]) == record[1] else 0)\n",
    "\n",
    "    # Step 2: Use reduce to sum up the correct predictions\n",
    "    correctly_classified = prediction_results.reduce(lambda a, b: a + b)\n",
    "\n",
    "    # Step 4: Calculate accuracy\n",
    "    accuracy = correctly_classified / RDD_xy.count()\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:07:05.160078Z",
     "start_time": "2023-11-28T14:07:05.146390Z"
    }
   },
   "id": "ccceadbe3201fdb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing\n",
    "\n",
    "The following code snippet is used for testing the whole system:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3da22ce1e89265b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 1.6187735561216878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 0.7508907786587498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Cost: 0.47055519879066693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Cost: 0.3873216955002531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Cost: 0.3558542271726476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Cost: 0.3414883097603333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Cost: 0.33415582134112765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Cost: 0.3301571193448983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, Cost: 0.32788337723046357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Cost: 0.3265529557781806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.928756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from preprocess import readFile\n",
    "from train import train\n",
    "from normalize import normalize\n",
    "from accuracy import accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read data\n",
    "    data = readFile(\"data/botnet_tot_syn_l.csv\")\n",
    "    # standardize\n",
    "    data = normalize(data)\n",
    "    # train\n",
    "    weights, bias = train(data, 10, 1.5, 0.05)\n",
    "    # accuracy\n",
    "    accuracy = accuracy(weights, bias, data)\n",
    "    print(\"accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:09:44.156461Z",
     "start_time": "2023-11-28T14:09:10.976068Z"
    }
   },
   "id": "aab4b79255387ba5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
